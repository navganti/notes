---
aliases: [Tokenization]
tags: [machine_learning]
---
# Tokenization

Tokenization is a preprocessing step for NLP which turns text into tokens. From here, tokens can then be converted into numbers and tensors.

Some examples of tokenization include splitting words based on sentences or punctuation.

A *vocabulary* is the number of independent tokens in the corpus.

Similarly to computer vision preprocessing steps, tokenization can add padding or truncate sentences to ensure that examples are the same length and fit within model parameters.

## Related

[[mlTransformer|Transformer]]

## References

[[@huggingfaceTokenizers]]
